---
title: "Kaggle Competition"
author: "Alon Rashty & Yuval Rittberg"
date: "6/31/2021"
output:
  html_document: 
    theme: readable
    toc: yes
    toc_depth: 2
    toc_float: yes
    code_folding: show
editor_options: 
  chunk_output_type: console
---
```{r, setup, include=FALSE}
knitr::opts_chunk$set(message=FALSE, warning = FALSE)
```


# Packages & Functions
```{r , message = FALSE}
if (!require("pacman")) install.packages("pacman")
  pacman::p_load(
    # For data manipulation, analysis and visualization
    tidyverse,
    DataExplorer,
    kableExtra,
    papeR,

    # For modeling
    tidymodels,
    stacks,
    glmnet,
    randomForest,
    xgboost,
    vip,
    
    # Other
    here,
    remotes,
    doParallel
    )
  
  pacman::p_load_gh("Sorenson-Impact/sorensonimpact")
```

```{r}
# Function to detect binary variables
is_binary <- function(x) {
  x0 <- na.omit(x)
  is.numeric(x) && length(unique(x0)) %in% 1:2 && all(x0 %in% 0:1)
}

# Function for table formatting
kable_format <- function(x, digits = 1, caption = NULL, position = "center") {
  kbl(x = x, digits = digits, caption = caption) %>% 
  row_spec(row = 0,bold = TRUE) %>% 
  kable_classic_2(lightable_options = c("bordered", "hover"), full_width = F, 
                  html_font = "Cambria",position = position
                  )
} 
```

# Data
```{r, message=FALSE}
train_raw <- read_csv(here("Kaggle","train.csv"))

train <- train_raw %>% select(-ID) 

test <- read_csv(here("Kaggle","test.csv"))
```

## First look

```{r, message=FALSE}
summarise(train, count = FALSE) %>% kable_format(digits = 2)
```

We can observe some immediate details from this summary:

* The feature `farmer` is always zero, so it'll be dropped later in the recipe.
* There are several very sparse dummies in the race and occupation features. We might want to unite some of them.
* The min value of `edyrs` is 0, which seems strange and a potential outlier.

Here we construct some categorical features for easier comparison in the following sections.
```{r, clas}
# Education, region, and race
train <- train %>% 
  mutate(in_school = as_factor(if_else(edyrs %in% c(13:15, 17), 1, 0)),
         educ = fct_case_when(
           edyrs < 12          ~ "non-graduate",
           edyrs == 12         ~ "high school",
           edyrs %in% c(13:16) ~ "college",
           edyrs > 16          ~ "advanced"),
         region = case_when(
           northeast==1    ~ "northeast",
           northcentral==1 ~ "northcentral",
           south==1        ~ "south", 
           TRUE            ~ "west"),
         race = case_when(
           black==1     ~ "black", 
           hisp==1      ~ "hisp", 
           otherrace==1 ~ "otherrace", 
           TRUE         ~ "white")
         )

# Occupations
occupations <- c("manager", "business", "financialop", "computer", "architect", "scientist", "socialworker", "postseceduc", "legaleduc", "artist", "lawyerphysician", "healthcare", "healthsupport", "protective", "foodcare", "building", "sales", "officeadmin", "constructextractinstall", "production", "transport")

train <- train %>% 
  select(all_of(occupations), -farmer) %>% 
  sweep(2, c(1:length(occupations)), "*") %>% 
  mutate(occupation = rowSums(.)) %>%
  select(occupation) %>% 
  bind_cols(train) 

train$occupation <- factor(train$occupation, labels = occupations)
```

## Experience

### Full-time
```{r}
train %>% ggplot(aes(expf)) +
  geom_histogram()

train %>% ggplot(aes(x = expf, y = lnwage)) +
  geom_jitter() +
  geom_smooth()
```

Full-time experience is correlated with income, but mostly at the beginning of the career, so we should also use `expfsq`.

### Part-time
```{r}
train %>% ggplot(aes(expp)) +
  geom_histogram()

train %>% ggplot(aes(x = expp, y = lnwage)) +
  geom_jitter()
```

About a half of our sample don't have any part-time experience, and the numbers are quite low for the rest. It is also uncorrelated with our outcome so we might not use it, or give it less weight relative to full-time experience.

## Gender
```{r}
train %>% ggplot(aes(x = as_factor(female), y = lnwage, fill = as_factor(female))) +
  geom_boxplot()

train %>% ggplot(aes(x = as_factor(female), y = edyrs, fill = as_factor(female))) +
  geom_boxplot()

train %>% ggplot(aes(x = as_factor(female), y = expf, fill = as_factor(female))) +
  geom_boxplot()
```

Gender is obviously (and unfortunately) correlated with income and experience, but not with education.

## Education
Let's look at the feature `edyrs`:
```{r}
train %>% ggplot(aes(edyrs)) + 
  geom_histogram(binwidth = 1)
```

We can see that there's a relatively significant concentration at the value of 12, 16 and 18, which represent finishing high school, college or an advanced degree, respectively. 

There shouldn't be an important return to education for people who didn't finish high-school:
```{r}
train %>% filter(edyrs!=0) %>% 
  ggplot(aes(x = edyrs, y = lnwage))+
  geom_jitter() + 
  geom_smooth()
```

We see that there's a shift for additional education after high school, but not below it, so we'll add a dummy for not finishing high school. 

```{r}
train %>% 
  ggplot(aes(x = educ, y = lnwage, fill = as_factor(educ))) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  theme(legend.position = "hide") 
```

However, here it seems less distinct.

Now we address the min value of `edyrs` which we mentioned earlier:
```{r}
train %>% filter(edyrs==0) %>% kable_format() %>% scroll_box(width = "100%")
```

There is one observation with `edyrs==0`. He is Hispanic, male, construction worker. We might consider drop him.



## Occupation

```{r, fig.height=10}
n_occ <- train %>% count(occupation)

# lnwage boxplot
wage_occ_boxplot <- train %>% 
  ggplot(aes(x = reorder(occupation, lnwage, FUN = median), y = lnwage, fill = as_factor(occupation))) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  geom_text(data = n_occ, aes(x =occupation, y = 1, label = paste0("N=",n)), size = 3, angle = 30) +
  ggtitle("Income") +
  theme(legend.position = "hide",
        axis.text.x = element_text(size=8, angle=45),
        axis.title.x = element_blank()) 

# edyrs boxplot
edu_occ_boxplot <- train %>% ggplot(aes(x = reorder(occupation, edyrs, FUN = median), y = edyrs, fill = as_factor(occupation))) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  geom_text(data = n_occ, aes(x =occupation, y = 2, label = paste0("N=",n)), size = 3, angle = 30) +
  ggtitle("Education") +
  theme(legend.position = "hide", 
        axis.text.x = element_text(size=8, angle=45),
        axis.title.x = element_blank())

grid.arrange(wage_occ_boxplot, edu_occ_boxplot)
```

* We see that there's a difference in the median income and education between occupations, and that these stats are in large correlated for each occupation. We can see the the pink-purple occupations ("low-skilled") are on the left and that the green-browns are on the right ("high-skilled"), in both plots. Therefore, we might unite them to groups which capture these differences, and overcome the sparseness.

* For some occupations such as `manager` and `sales` these stats are quite volatile, which might suggest they are not so informative for prediction. For example, in `transport` we might have pilots and bus drivers together.

* There are some extreme outliers, such as in `artist`'s income, which has the highest income in the sample, maybe a movie star.

* Again, we see the sparse variables:
`artist`, `computer`, `financialop`, `business`, `postseceduc`, `lawyerphysician`, `socialworker`, `scientist` and `protective`. We should unite them with other variables which might represent similar people.

## Regions
```{r}
n_region <- train %>% count(region)

region_lnwage_boxplot <- train %>% 
  ggplot(aes(x = region, y = lnwage, fill = as_factor(region))) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  geom_text(data = n_region, aes(x = region, y = 1, label = paste0("N=",n)), size = 3) +
  ggtitle("Income") +
  theme(legend.position = "hide",
        axis.title.x = element_blank()) 

region_edyrs_boxplot <- train %>% 
  ggplot(aes(x = region, y = edyrs, fill = as_factor(region))) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  geom_text(data = n_region, aes(x = region, y = 1, label = paste0("N=",n)), size = 3) +
  ggtitle("Education") +
  theme(legend.position = "hide",
        axis.title.x = element_blank())   

grid.arrange(region_lnwage_boxplot, region_edyrs_boxplot, ncol = 2)
```

We can see that the south is distinct from the other regions, which are quite similar. Maybe keeping only the `south` feature would be more informative.

## Race
```{r}
n_race <- train %>% count(race)

race_lnwage_boxplot <- train %>% 
  ggplot(aes(x = race, y = lnwage, fill = race)) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  geom_text(data = n_race, aes(x = race, y = 1, label = paste0("N=",n)), size = 3) +
  ggtitle("Income") +
  theme(legend.position = "hide",
        axis.title.x = element_blank())   

race_educ_boxplot <- train %>% 
  ggplot(aes(x = race, y = edyrs, fill = race)) +
  stat_boxplot(geom = "errorbar") +
  geom_boxplot() +
  geom_text(data = n_race, aes(x = race, y = 1, label = paste0("N=",n)), size = 3) +
  ggtitle("Education") +
  theme(legend.position = "hide",
        axis.title.x = element_blank())  

grid.arrange(race_lnwage_boxplot, race_educ_boxplot, ncol = 2)
```

Here we can more or less distinguish between `black` + `hisp` and `white` + `otherrace`. We should replace the current race features with that division.

# Modelling
```{r}
set.seed(2021)

# Cross validation folds
cv_folds <- train_raw %>%
  vfold_cv(v = 5) # can also add repeats and strata

# For stacking
ctrl_grid <- control_stack_grid()

# Recipe (data processing)
train_rec <- recipe(lnwage ~ ., data = train_raw %>% select(-ID)) %>%
  step_mutate(total_exp    = expf + 0.5*expp,
              total_expsq  = total_exp^2,
              non_graduate = if_else(edyrs<12, 1, 0),
              high_school  = if_else(edyrs>=12, 1, 0), # maybe between 12 and 16?
              white_other  = if_else(black==0 & hisp==0, 1, 0),
              academic     = if_else(edyrs>=16, 1, 0), 
              skilled      = if_else(foodcare==1 | healthsupport==1 | building==1 | sales==1 | officeadmin==1 |
                                     production==1 | transport==1 | constructextractinstall==1, 0, 1)
              ) %>% 
  step_normalize(edyrs, total_exp, total_expsq) %>%
  
  # step_rm(starts_with("exp"), manager, business, financialop, computer, architect, scientist,
  #         socialworker, postseceduc, legaleduc, artist, lawyerphysician, healthcare, healthsupport,
  #         protective, foodcare, building, sales, officeadmin, constructextractinstall, production,
  #         transport, northeast, northcentral, black ,hisp, otherrace) %>%
  
  #step_interact(~ all_predictors():all_predictors()) %>%
  
  step_zv(all_predictors())

# Enhancing performance
registerDoParallel()
```


```{r}

```


## LASSO
```{r}
# Model definition
lasso_model <-
  linear_reg(penalty = tune(), mixture = 1) %>%
    set_engine("glmnet") %>%
    set_mode("regression")

# Define parameters for tuning
lasso_grid <- grid_regular(penalty(), levels = 50)

# Combine models and workflow
lasso_wf <- workflow() %>%
  add_recipe(train_rec) %>%
  add_model(lasso_model)

# Tune parameters
lasso_results <- lasso_wf %>%
  tune_grid(grid = lasso_grid,
            resamples = cv_folds,
            control = ctrl_grid)

lasso_results %>% show_best(metric = "rmse") %>% select(-c(n, .config)) %>% kable_format(digits = 4)

# Visualize results
lasso_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
    ggplot(aes(penalty, mean)) +
      geom_errorbar(aes(
        ymin = mean - std_err,
        ymax = mean + std_err)
        ) +
      geom_line(size = 1.5) +
      scale_x_log10() +
      theme(legend.position = "none")

# Select best model
lasso_best <- lasso_results %>%
select_best(metric = "rmse")

# Finalize workflow
lasso_final <- finalize_workflow(lasso_wf, lasso_best)
lasso_model_final <- finalize_model(lasso_model, lasso_best)
lasso_final_fit <- fit(lasso_model_final, lnwage ~ ., data = train_raw)

# Variable importance
lasso_final %>% fit(data = train) %>% pull_workflow_fit() %>% vip(geom = "col")

```


## Random Forests
```{r}
# Define model
rf_model <-
  rand_forest(mtry = tune(),
              trees = tune(),
              min_n = tune()
              ) %>%
  set_engine("randomForest") %>%
  set_mode("regression")

# Define parameters for tuning
rf_grid <- grid_latin_hypercube(
  mtry(c(8, 20)),
  trees(c(500, 2000)),
  min_n(c(20, 55)),
  size = 20
  )

# Combine model and recipe
rf_wf <-
  workflow() %>%
  add_recipe(train_rec) %>%
  add_model(rf_model)

# Tune parameters
rf_results <-
  tune_grid(rf_wf,
            resamples = cv_folds,
            grid = rf_grid,
            control = ctrl_grid
            )

# Evaluate tuning results
show_best(rf_results, "rmse", n = 5) %>% select(-c(.estimator, .config, n)) %>% kable_format(digits = 3)

# Visualize results
rf_results %>%
  collect_metrics() %>%
  filter(.metric == "rmse") %>%
  select(mean, mtry:min_n) %>%
  pivot_longer(mtry:min_n,
               values_to = "value",
               names_to = "parameter"
               ) %>%
  ggplot(aes(value, mean, color = parameter)) +
  geom_point(alpha = 0.8, show.legend = FALSE) +
  facet_wrap(~parameter, scales = "free_x") +
  labs(x = NULL, y = "rmse")

# Select best model
rf_best <- select_best(rf_results, metric = "rmse")

# Finalize workflow
rf_final <- finalize_workflow(rf_wf, rf_best)
rf_model_final <- finalize_model(rf_model, rf_best)
rf_final_fit <- fit(rf_model_final, lnwage ~ ., data = train_raw)

# Variable importance
rf_final %>% fit(data = train) %>% pull_workflow_fit() %>% vip(geom = "col")


```

## XGBoost
```{r}
# Define model
xgb_model <-
  boost_tree(mtry = tune(),           # number of predictors to choose from at each split (for randomness)
             trees = tune(),          # number of trees (complexity)
             min_n = tune(),          # minimum obs in a node (complexity)
             tree_depth = tune(),     # max number of split (complexity)
             learn_rate = tune(),     # step size
             loss_reduction = tune(), # the reduction in the loss function required to split further
             sample_size = tune()     # fraction of data in each round (for randomness)
            ) %>%
  set_mode("regression") %>%
  set_engine("xgboost", objective = "reg:squarederror")

# Define parameters for tuning
xgb_grid <- grid_latin_hypercube(
  mtry(c(5, 20)),
  trees(c(500, 2000)),
  min_n(c(20, 100)),
  tree_depth(),
  learn_rate(c(-2.5,-1.5)),
  loss_reduction(c(-9, 1)),
  sample_size = sample_prop(c(0.6, 0.9)),
  size = 30
  )

# Combine model and recipe
xgb_wf <-
workflow() %>%
add_recipe(train_rec) %>%
add_model(xgb_model)

# Tune parameters
xgb_results <-
tune_grid(xgb_wf,
          resamples = cv_folds,
          grid = xgb_grid,
          control = ctrl_grid
          )

# Evaluate tuning results
show_best(xgb_results, "rmse") %>% select(-c(.estimator, .config, n)) %>% kable_format(digits = 3)

# Visualize results
xgb_results %>%
collect_metrics() %>%
filter(.metric == "rmse") %>%
select(mean, mtry:sample_size) %>%
pivot_longer(mtry:sample_size,
             values_to = "value",
             names_to = "parameter"
             ) %>%
ggplot(aes(value, mean, color = parameter)) +
geom_point(alpha = 0.8, show.legend = FALSE) +
facet_wrap(~parameter, scales = "free_x") +
labs(x = NULL, y = "rmse")

# Select best model
xgb_best <- select_best(xgb_results, metric = "rmse")

# Finalize workflow
xgb_final <- finalize_workflow(xgb_wf, xgb_best)
xgb_model_final <- finalize_model(xgb_model, xgb_best)
xgb_final_fit <- fit(xgb_model_final, lnwage ~ ., data = train_raw)

# Variable importance
xgb_final %>% fit(data = train) %>% pull_workflow_fit() %>% vip(geom = "col")
```

## Stacking models
At this stage we will combine all our models to gets the best out of them, using [`{stacks}`](https://stacks.tidymodels.org/index.html) package:

> _"stacks is an R package for model stacking that aligns with the tidymodels. Model stacking is an ensembling method that takes the outputs of many models and combines them to generate a new model—referred to as an ensemble in this package—that generates predictions informed by each of its members."_

```{r}
stack <-
  stacks() %>%
  add_candidates(lasso_results) %>%
  add_candidates(rf_results) %>%
  add_candidates(xgb_results)

stack_model <-
  stack %>%
  blend_predictions()

autoplot(stack_model)
autoplot(stack_model, type = "members")
autoplot(stack_model, type = "weights")

stack_model <- stack_model %>% fit_members()
```

# Prediction & Submission
```{r}
stack_pred <-
  predict(stack_model, new_data = test) %>%
    rename(lnwage = .pred) %>% 
    bind_cols(select(test, ID)) %>% 
    select(ID, lnwage)

write_csv(stack_pred, here("Kaggle", "submission.csv"))
```

